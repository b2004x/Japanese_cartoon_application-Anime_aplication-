{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4fb524",
   "metadata": {},
   "source": [
    "Embedding Sypnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca7e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4fc57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime = pd.read_csv(r\"G:\\hoc\\private\\Anime\\data\\Recomended_Anime_data\\raw\\MyAnimeList-Database-master\\data\\anime_with_synopsis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e72810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea598be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ef493817194082ba2dcbfed6f93d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(df_anime[\"sypnopsis\"].tolist(), show_progress_bar=True, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r\"G:\\hoc\\private\\Anime\\notebooks\\Recomended_Anime_data/anime_embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9fe0f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.104.2-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in g:\\hoc\\anacoda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in g:\\hoc\\anacoda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in g:\\hoc\\anacoda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in g:\\hoc\\anacoda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in g:\\hoc\\anacoda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in g:\\hoc\\anacoda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in g:\\hoc\\anacoda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.104.2-py3-none-any.whl (928 kB)\n",
      "   ---------------------------------------- 0.0/928.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 928.2/928.2 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.10.0 openai-1.104.2\n",
      "Requirement already satisfied: openai in g:\\hoc\\anacoda\\lib\\site-packages (1.104.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in g:\\hoc\\anacoda\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in g:\\hoc\\anacoda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in g:\\hoc\\anacoda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in g:\\hoc\\anacoda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in g:\\hoc\\anacoda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in g:\\hoc\\anacoda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in g:\\hoc\\anacoda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in g:\\hoc\\anacoda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b3a8b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_OPE*******_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size):\n\u001b[0;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m---> 22\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     23\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mbatch\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     27\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n",
      "File \u001b[1;32mg:\\hoc\\anacoda\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    135\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    136\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    137\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    138\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    139\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    140\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    141\u001b[0m     ),\n\u001b[0;32m    142\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    143\u001b[0m )\n",
      "File \u001b[1;32mg:\\hoc\\anacoda\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mg:\\hoc\\anacoda\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_OPE*******_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Load dataset\n",
    "# df_anime = pd.read_csv(\n",
    "#     r\"G:\\hoc\\private\\Anime\\data\\Recomended_Anime_data\\raw\\MyAnimeList-Database-master\\data\\anime_with_synopsis.csv\"\n",
    "# )\n",
    "\n",
    "# # Initialize OpenAI client\n",
    "# client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "\n",
    "# # Extract the synopsis column (note: your column seems to be misspelled \"sypnopsis\")\n",
    "# texts = df_anime[\"sypnopsis\"].fillna(\"\").tolist()\n",
    "\n",
    "# # Generate embeddings in batches (to avoid hitting token/input limits)\n",
    "# batch_size = 1000\n",
    "# embeddings = []\n",
    "\n",
    "# for i in range(0, len(texts), batch_size):\n",
    "#     batch = texts[i:i+batch_size]\n",
    "#     response = client.embeddings.create(\n",
    "#         model=\"text-embedding-3-large\",\n",
    "#         input=batch\n",
    "#     )\n",
    "#     batch_embeddings = [item.embedding for item in response.data]\n",
    "#     embeddings.extend(batch_embeddings)\n",
    "\n",
    "# # Convert to numpy array and save\n",
    "# embeddings = np.array(embeddings)\n",
    "# np.save(r\"G:\\hoc\\private\\Anime\\notebooks\\Recomended_Anime_data\\text-embedding-3-large.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cowboy Bebop: Tengoku no Tobira', 0.67239034),\n",
       " ('Ginga Reppuu Baxingar', 0.594574),\n",
       " ('Xevious', 0.5573846),\n",
       " ('Macross F', 0.55287504),\n",
       " ('Star Fox Zero: The Battle Begins', 0.5510756)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238300d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc80ac3c",
   "metadata": {},
   "source": [
    "<p style=\"font-size:40px; color:White;\">II. Calling embedding npz</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in g:\\hoc\\anacoda\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in g:\\hoc\\anacoda\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/18.2 MB 2.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.3/18.2 MB 2.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.8/18.2 MB 2.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.4/18.2 MB 2.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.6/18.2 MB 2.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.1/18.2 MB 2.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.7/18.2 MB 2.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 4.2/18.2 MB 2.3 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 4.5/18.2 MB 2.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 5.0/18.2 MB 2.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 5.0/18.2 MB 2.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.2/18.2 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.2/18.2 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.5/18.2 MB 1.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.8/18.2 MB 1.7 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 5.8/18.2 MB 1.7 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 6.0/18.2 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 6.6/18.2 MB 1.6 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 6.8/18.2 MB 1.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 7.3/18.2 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 7.6/18.2 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 8.1/18.2 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 8.4/18.2 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 8.7/18.2 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.2/18.2 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 9.4/18.2 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 10.0/18.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 10.2/18.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 10.7/18.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 11.3/18.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.8/18.2 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 12.3/18.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.8/18.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 13.4/18.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 13.9/18.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.4/18.2 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 14.9/18.2 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.2/18.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.2/18.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.5/18.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.7/18.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.0/18.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.3/18.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.5/18.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.8/18.2 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.2 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.6/18.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c4441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def recommend_by_embedding(Name, anime_df, embeddings, top_n=5):\n",
    "    idx = anime_df.index[anime_df[\"Name\"] == Name][0]\n",
    "    query_vec = embeddings[idx].reshape(1, -1)\n",
    "    \n",
    "    sim_scores = cosine_similarity(query_vec, embeddings)[0]\n",
    "    top_idx = sim_scores.argsort()[-top_n-1:][::-1]  # sort high → low\n",
    "    \n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if i != idx:  # skip the anime itself\n",
    "            results.append((anime_df.iloc[i][\"Name\"], sim_scores[i]))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb8a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime = pd.read_csv(r\"G:\\hoc\\private\\Anime\\data\\Recomended_Anime_data\\raw\\MyAnimeList-Database-master\\data\\anime_with_synopsis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1224a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"anime_embeddings.npy\").astype(\"float32\")\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Build FAISS index\n",
    "d = embeddings.shape[1]  # embedding dimension\n",
    "index = faiss.IndexFlatIP(d)  # Inner Product works with normalized vectors\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"anime_index.faiss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e06b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Name Score  \\\n",
      "1508                           Naruto: Shippuuden  8.16   \n",
      "11346             Boruto: Naruto Next Generations  5.81   \n",
      "5487    Naruto: Shippuuden Movie 5 - Blood Prison  7.46   \n",
      "8831                     Boruto: Naruto the Movie   7.5   \n",
      "6158   Naruto: Shippuuden Movie 6 - Road to Ninja  7.67   \n",
      "\n",
      "                                                  Genres  \n",
      "1508   Action, Adventure, Comedy, Super Power, Martia...  \n",
      "11346  Action, Adventure, Super Power, Martial Arts, ...  \n",
      "5487   Action, Adventure, Martial Arts, Super Power, ...  \n",
      "8831   Action, Comedy, Martial Arts, Shounen, Super P...  \n",
      "6158   Action, Adventure, Super Power, Martial Arts, ...  \n"
     ]
    }
   ],
   "source": [
    "faiss.normalize_L2(embeddings)\n",
    "embeddings = np.load(\"anime_embeddings.npy\").astype(\"float32\")\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"anime_index.faiss\")\n",
    "\n",
    "def recommend_faiss(anime_idx, k=5):\n",
    "    query = embeddings[anime_idx].reshape(1, -1).astype(\"float32\")\n",
    "    faiss.normalize_L2(query)  # normalize query too\n",
    "    scores, indices = index.search(query, k+1)  # k+1 to skip itself\n",
    "    return df_anime.iloc[indices[0][1:k+1]][[\"Name\", \"Score\", \"Genres\"]]\n",
    "\n",
    "# Example: recommend 5 similar anime to anime at index 10\n",
    "print(recommend_faiss(10, k=5))\n",
    "\n",
    "# cos_sim = cosine_similarity(embeddings)\n",
    "# recommend_by_embedding(\"Cowboy Bebop\", df_anime, embeddings, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b98b7ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_idx = {Name: i for i, Name in enumerate(df_anime[\"Name\"])}\n",
    "embeddings = np.load(\"anime_embeddings.npy\").astype(\"float32\")\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"anime_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05e2c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_name(title, k=5):\n",
    "    if title not in Name_to_idx:\n",
    "        raise ValueError(f\"'{title}' not found in dataset\")\n",
    "\n",
    "    # Get index of this anime\n",
    "    anime_idx = Name_to_idx[title]\n",
    "\n",
    "    # Build query vector\n",
    "    query = embeddings[anime_idx].reshape(1, -1).astype(\"float32\")\n",
    "    faiss.normalize_L2(query)\n",
    "\n",
    "    # Search in FAISS\n",
    "    scores, indices = index.search(query, k+1)  # k+1 to skip itself\n",
    "    indices = indices[0][1:]   # drop itself\n",
    "    scores = scores[0][1:]\n",
    "\n",
    "    # Return DataFrame with similarity scores\n",
    "    result = df_anime.iloc[indices][[\"Name\", \"Score\", \"Genres\"]].copy()\n",
    "    result[\"similarity\"] = scores\n",
    "    return result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c4d69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Name    Score  \\\n",
      "0            Initial D First Stage     8.29   \n",
      "1            Initial D Third Stage     7.89   \n",
      "2  Arrow Emblem Grand Prix no Taka     6.47   \n",
      "3           Initial D Second Stage     8.12   \n",
      "4         Tobidase! Machine Hiryuu  Unknown   \n",
      "\n",
      "                                         Genres  similarity  \n",
      "0           Action, Cars, Drama, Seinen, Sports    0.607040  \n",
      "1  Action, Cars, Sports, Drama, Romance, Seinen    0.597591  \n",
      "2                           Cars, Sports, Drama    0.580465  \n",
      "3           Action, Cars, Drama, Seinen, Sports    0.577739  \n",
      "4                 Cars, Comedy, Parody, Shounen    0.556803  \n"
     ]
    }
   ],
   "source": [
    "print(recommend_by_name(\"Initial D Fifth Stage\", k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e99772e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf4d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process\n",
    "\n",
    "def find_closest_title(query, titles, limit=1, score_cutoff=70):\n",
    "    \"\"\"\n",
    "    Fuzzy match a user query to the closest title in the dataset.\n",
    "    \"\"\"\n",
    "    matches = process.extract(query, titles, limit=limit, score_cutoff=score_cutoff)\n",
    "    if not matches:\n",
    "        return None\n",
    "    # return best match\n",
    "    return matches[0][0]   # (title, score, index) → take title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833da3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_name_fuzzy(query, k=5):\n",
    "    # Find closest anime title in dataset\n",
    "    best_match = find_closest_title(query, df_anime[\"Name\"].tolist())\n",
    "    if best_match is None:\n",
    "        raise ValueError(f\"No close match found for '{query}'\")\n",
    "\n",
    "    # Lookup index\n",
    "    anime_idx = Name_to_idx[best_match]\n",
    "\n",
    "    # Build query vector\n",
    "    qvec = embeddings[anime_idx].reshape(1, -1).astype(\"float32\")\n",
    "    faiss.normalize_L2(qvec)\n",
    "\n",
    "    # Search FAISS\n",
    "    scores, indices = index.search(qvec, k+1)\n",
    "    indices = indices[0][1:]\n",
    "    scores = scores[0][1:]\n",
    "\n",
    "    # Return results\n",
    "    result = df_anime.iloc[indices][[\"Name\", \"Score\", \"Genres\"]].copy()\n",
    "    result[\"similarity\"] = scores\n",
    "    return best_match, result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7fa2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You searched for: Love Live\n",
      "Best match in dataset: Love Live! School Idol Project\n",
      "                                                Name Score  \\\n",
      "0          Love Live! School Idol Project 2nd Season  7.81   \n",
      "1                   Love Live! The School Idol Movie  7.95   \n",
      "2  Love Live! Nijigasaki Gakuen School Idol Douko...  7.62   \n",
      "3                              Love Live! Sunshine!!   7.4   \n",
      "4                   Love Live! Sunshine!! 2nd Season  7.58   \n",
      "\n",
      "                         Genres  similarity  \n",
      "0  Music, School, Slice of Life    0.737099  \n",
      "1  Music, School, Slice of Life    0.697409  \n",
      "2  Music, Slice of Life, School    0.675268  \n",
      "3  Music, Slice of Life, School    0.655351  \n",
      "4  Music, School, Slice of Life    0.640358  \n"
     ]
    }
   ],
   "source": [
    "query = \"Love Live\"   # user typo or different wording\n",
    "match, recs = recommend_by_name_fuzzy(query, k=5)\n",
    "\n",
    "print(\"You searched for:\", query)\n",
    "print(\"Best match in dataset:\", match)\n",
    "print(recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
