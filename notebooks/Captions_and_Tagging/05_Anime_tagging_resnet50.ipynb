{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46117f18",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-21T16:56:08.653474Z",
     "iopub.status.busy": "2025-10-21T16:56:08.652941Z",
     "iopub.status.idle": "2025-10-21T16:58:09.647457Z",
     "shell.execute_reply": "2025-10-21T16:58:09.646826Z"
    },
    "papermill": {
     "duration": 121.000705,
     "end_time": "2025-10-21T16:58:09.648925",
     "exception": false,
     "start_time": "2025-10-21T16:56:08.648220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "caption_dir = '/kaggle/input/tagged-anime-illustrations/danbooru-metadata/danbooru-metadata'\n",
    "\n",
    "id_to_tags = {}\n",
    "\n",
    "for filename in os.listdir(caption_dir):\n",
    "    f_path = os.path.join(caption_dir, filename)\n",
    "    if os.path.isfile(f_path):\n",
    "        with open(f_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.rstrip())\n",
    "                tags = [x[\"name\"] for x in data[\"tags\"]]\n",
    "                caption = \" \".join(tags)\n",
    "                id_to_tags[data[\"id\"]] = caption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c03995c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:58:09.657049Z",
     "iopub.status.busy": "2025-10-21T16:58:09.656460Z",
     "iopub.status.idle": "2025-10-21T16:58:56.561145Z",
     "shell.execute_reply": "2025-10-21T16:58:56.560538Z"
    },
    "papermill": {
     "duration": 46.909806,
     "end_time": "2025-10-21T16:58:56.562558",
     "exception": false,
     "start_time": "2025-10-21T16:58:09.652752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_counter = Counter()\n",
    "for tags in id_to_tags.values():\n",
    "    for tag in tags.split(\" \"):\n",
    "        tag = tag.strip()\n",
    "        if tag:  # avoid empty strings\n",
    "            tag_counter[tag] += 1\n",
    "            \n",
    "top_tags = [tag for tag, _ in tag_counter.most_common(500)]\n",
    "\n",
    "top_tags_set = set(top_tags)\n",
    "\n",
    "top_2000_id_to_tags = {\n",
    "    img_id: \" \".join([t for t in tags.split(\" \") if t in top_tags_set])\n",
    "    for img_id, tags in id_to_tags.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872b34b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:58:56.570051Z",
     "iopub.status.busy": "2025-10-21T16:58:56.569800Z",
     "iopub.status.idle": "2025-10-21T16:59:16.930533Z",
     "shell.execute_reply": "2025-10-21T16:59:16.929610Z"
    },
    "papermill": {
     "duration": 20.365731,
     "end_time": "2025-10-21T16:59:16.931821",
     "exception": false,
     "start_time": "2025-10-21T16:58:56.566090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tags: 500\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "filtered_tag_counter_2000 = Counter()\n",
    "\n",
    "for tags in top_2000_id_to_tags.values():\n",
    "    for tag in tags.split(\" \"):\n",
    "        tag = tag.strip()\n",
    "        if tag:  \n",
    "            filtered_tag_counter_2000[tag] += 1\n",
    "\n",
    "print(\"Total unique tags:\", len(filtered_tag_counter_2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4994260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:59:16.938929Z",
     "iopub.status.busy": "2025-10-21T16:59:16.938684Z",
     "iopub.status.idle": "2025-10-21T16:59:16.942553Z",
     "shell.execute_reply": "2025-10-21T16:59:16.941899Z"
    },
    "papermill": {
     "duration": 0.008811,
     "end_time": "2025-10-21T16:59:16.943923",
     "exception": false,
     "start_time": "2025-10-21T16:59:16.935112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1girl bow brown_hair detached_sleeves frills hair_bow hair_ribbon hair_tubes hakurei_reimu highres midriff navel red_eyes ribbon skirt skirt_set solo standing touhou\n"
     ]
    }
   ],
   "source": [
    "print(top_2000_id_to_tags[\"1017000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69413b16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:59:16.950845Z",
     "iopub.status.busy": "2025-10-21T16:59:16.950616Z",
     "iopub.status.idle": "2025-10-21T16:59:16.953570Z",
     "shell.execute_reply": "2025-10-21T16:59:16.953089Z"
    },
    "papermill": {
     "duration": 0.007563,
     "end_time": "2025-10-21T16:59:16.954569",
     "exception": false,
     "start_time": "2025-10-21T16:59:16.947006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_tags = list(filtered_tag_counter_2000.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fb0d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:59:16.961481Z",
     "iopub.status.busy": "2025-10-21T16:59:16.961291Z",
     "iopub.status.idle": "2025-10-21T16:59:26.293515Z",
     "shell.execute_reply": "2025-10-21T16:59:26.292954Z"
    },
    "papermill": {
     "duration": 9.337215,
     "end_time": "2025-10-21T16:59:26.294858",
     "exception": false,
     "start_time": "2025-10-21T16:59:16.957643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class DanbooruMultiLabelDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dict, unique_tags, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: folder with all subfolders of images\n",
    "        label_dict: dict mapping 'image_id' -> list of tags\n",
    "        unique_tags: list of all unique tags (defines the multi-label space)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dict = label_dict\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "        self.transform = transform\n",
    "\n",
    "        # Collect image paths\n",
    "        self.image_paths = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            for f in files:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    img_id = os.path.splitext(f)[0]\n",
    "                    if img_id in label_dict:\n",
    "                        self.image_paths.append(os.path.join(subdir, f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def encode_tags(self, tags):\n",
    "        vec = torch.zeros(len(self.tag_to_idx), dtype=torch.float32)\n",
    "        for tag in tags:\n",
    "            if tag in self.tag_to_idx:\n",
    "                vec[self.tag_to_idx[tag]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        tags_str = self.label_dict[img_id]\n",
    "    \n",
    "        # Split tags if they are stored as a space-separated string\n",
    "        if isinstance(tags_str, str):\n",
    "            tags = tags_str.split()  # split by whitespace\n",
    "        else:\n",
    "            tags = tags_str  # already a list\n",
    "    \n",
    "        label_vec = self.encode_tags(tags)\n",
    "    \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "    \n",
    "        return img, label_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3607d662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:59:26.303384Z",
     "iopub.status.busy": "2025-10-21T16:59:26.303106Z",
     "iopub.status.idle": "2025-10-21T17:05:22.854685Z",
     "shell.execute_reply": "2025-10-21T17:05:22.853977Z"
    },
    "papermill": {
     "duration": 356.557798,
     "end_time": "2025-10-21T17:05:22.856156",
     "exception": false,
     "start_time": "2025-10-21T16:59:26.298358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 500])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = DanbooruMultiLabelDataset(\n",
    "    root_dir=\"/kaggle/input/tagged-anime-illustrations/danbooru-images/danbooru-images\",\n",
    "    label_dict=top_2000_id_to_tags,\n",
    "    unique_tags=unique_tags,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "for images, label_vecs in dataloader:\n",
    "    print(images.shape)      # (B, 3, 224, 224)\n",
    "    print(label_vecs.shape)  # (B, num_tags)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78b93c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:22.863942Z",
     "iopub.status.busy": "2025-10-21T17:05:22.863652Z",
     "iopub.status.idle": "2025-10-21T17:05:22.890230Z",
     "shell.execute_reply": "2025-10-21T17:05:22.889588Z"
    },
    "papermill": {
     "duration": 0.031654,
     "end_time": "2025-10-21T17:05:22.891323",
     "exception": false,
     "start_time": "2025-10-21T17:05:22.859669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 269626 | Val: 33703 | Test: 33704\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  \n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # reproducible splits\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdba2203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:22.898729Z",
     "iopub.status.busy": "2025-10-21T17:05:22.898508Z",
     "iopub.status.idle": "2025-10-21T17:05:25.044694Z",
     "shell.execute_reply": "2025-10-21T17:05:25.043839Z"
    },
    "papermill": {
     "duration": 2.151399,
     "end_time": "2025-10-21T17:05:25.046071",
     "exception": false,
     "start_time": "2025-10-21T17:05:22.894672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0 — Active tags: 12\n",
      "[14, 29, 60, 63, 113, 118, 141, 220, 335, 397, 444, 476]\n",
      "\n",
      "Sample 1 — Active tags: 17\n",
      "[2, 3, 13, 37, 41, 46, 73, 87, 104, 110, 113, 141, 145, 148, 446, 458, 464]\n",
      "\n",
      "Sample 2 — Active tags: 15\n",
      "[0, 3, 5, 38, 61, 96, 131, 135, 148, 175, 226, 339, 354, 416, 425]\n",
      "\n",
      "Sample 3 — Active tags: 21\n",
      "[2, 3, 14, 40, 41, 55, 79, 93, 95, 96, 98, 104, 147, 155, 185, 201, 311, 314, 322, 325, 344]\n",
      "\n",
      "Sample 4 — Active tags: 8\n",
      "[3, 5, 17, 90, 226, 269, 335, 360]\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the dataloader\n",
    "imgs, labels = next(iter(train_loader))\n",
    "\n",
    "# Print 5 label vectors and how many active tags they have\n",
    "for i in range(5):\n",
    "    label_vec = labels[i]\n",
    "    print(f\"\\nSample {i} — Active tags: {int(label_vec.sum().item())}\")\n",
    "    print(label_vec.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3abce11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:25.053855Z",
     "iopub.status.busy": "2025-10-21T17:05:25.053589Z",
     "iopub.status.idle": "2025-10-21T17:05:27.714478Z",
     "shell.execute_reply": "2025-10-21T17:05:27.713685Z"
    },
    "papermill": {
     "duration": 2.666497,
     "end_time": "2025-10-21T17:05:27.716032",
     "exception": false,
     "start_time": "2025-10-21T17:05:25.049535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/RF5/danbooru-pretrained/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://github.com/RF5/danbooru-pretrained/releases/download/v0.1/resnet50-13306192.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-13306192.pth\n",
      "100%|██████████| 110M/110M [00:00<00:00, 230MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the model\n",
    "model = torch.hub.load('RF5/danbooru-pretrained', 'resnet50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0888e732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.725202Z",
     "iopub.status.busy": "2025-10-21T17:05:27.724985Z",
     "iopub.status.idle": "2025-10-21T17:05:27.728239Z",
     "shell.execute_reply": "2025-10-21T17:05:27.727714Z"
    },
    "papermill": {
     "duration": 0.008907,
     "end_time": "2025-10-21T17:05:27.729278",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.720371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, transforms, models\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import copy\n",
    "# import os\n",
    "\n",
    "# # Load the model\n",
    "# num_classes = 500\n",
    "# model = models.resnet50(pretrained=True)\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de7d7e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.737278Z",
     "iopub.status.busy": "2025-10-21T17:05:27.737102Z",
     "iopub.status.idle": "2025-10-21T17:05:27.742269Z",
     "shell.execute_reply": "2025-10-21T17:05:27.741527Z"
    },
    "papermill": {
     "duration": 0.010379,
     "end_time": "2025-10-21T17:05:27.743352",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.732973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): AdaptiveConcatPool2d(\n",
      "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
      "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten()\n",
      "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=512, out_features=6000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da72bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.751937Z",
     "iopub.status.busy": "2025-10-21T17:05:27.751528Z",
     "iopub.status.idle": "2025-10-21T17:05:27.754468Z",
     "shell.execute_reply": "2025-10-21T17:05:27.753781Z"
    },
    "papermill": {
     "duration": 0.00832,
     "end_time": "2025-10-21T17:05:27.755536",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.747216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40eaa011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.764167Z",
     "iopub.status.busy": "2025-10-21T17:05:27.763973Z",
     "iopub.status.idle": "2025-10-21T17:05:27.814979Z",
     "shell.execute_reply": "2025-10-21T17:05:27.814264Z"
    },
    "papermill": {
     "duration": 0.056563,
     "end_time": "2025-10-21T17:05:27.816043",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.759480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Replaced final layer with Linear(512, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Linear(in_features=512, out_features=500, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_tags = len(unique_tags)\n",
    "# Replace final layer\n",
    "model[1][8] = nn.Linear(in_features=512, out_features=num_tags)\n",
    "print(f\"✅ Replaced final layer with Linear(512, {num_tags})\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebeda34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.825734Z",
     "iopub.status.busy": "2025-10-21T17:05:27.825128Z",
     "iopub.status.idle": "2025-10-21T17:05:27.828234Z",
     "shell.execute_reply": "2025-10-21T17:05:27.827556Z"
    },
    "papermill": {
     "duration": 0.00904,
     "end_time": "2025-10-21T17:05:27.829315",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.820275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Freeze feature extractor\n",
    "# for param in model[0].parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41030917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.838643Z",
     "iopub.status.busy": "2025-10-21T17:05:27.838128Z",
     "iopub.status.idle": "2025-10-21T17:05:27.841233Z",
     "shell.execute_reply": "2025-10-21T17:05:27.840558Z"
    },
    "papermill": {
     "duration": 0.008784,
     "end_time": "2025-10-21T17:05:27.842238",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.833454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b632f061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.851168Z",
     "iopub.status.busy": "2025-10-21T17:05:27.850972Z",
     "iopub.status.idle": "2025-10-21T17:05:27.854590Z",
     "shell.execute_reply": "2025-10-21T17:05:27.854093Z"
    },
    "papermill": {
     "duration": 0.009007,
     "end_time": "2025-10-21T17:05:27.855496",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.846489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCEWithLogitsLoss()  # multi-label classification\n",
    "optimizer = optim.Adam(model[1].parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90c18398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.864345Z",
     "iopub.status.busy": "2025-10-21T17:05:27.864146Z",
     "iopub.status.idle": "2025-10-21T17:05:27.869059Z",
     "shell.execute_reply": "2025-10-21T17:05:27.868355Z"
    },
    "papermill": {
     "duration": 0.010576,
     "end_time": "2025-10-21T17:05:27.870182",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.859606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tag-to-index mapping to tag_to_index_mapping.txt\n"
     ]
    }
   ],
   "source": [
    "output_path = \"tag_to_index_mapping.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for tag, idx in sorted(dataset.tag_to_idx.items(), key=lambda x: x[1]):\n",
    "        f.write(f\"{idx}\\t{tag}\\n\")\n",
    "\n",
    "print(f\"Saved tag-to-index mapping to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d27b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T17:05:27.879684Z",
     "iopub.status.busy": "2025-10-21T17:05:27.879483Z",
     "iopub.status.idle": "2025-10-22T02:15:03.686972Z",
     "shell.execute_reply": "2025-10-22T02:15:03.686053Z"
    },
    "papermill": {
     "duration": 32975.813885,
     "end_time": "2025-10-22T02:15:03.688205",
     "exception": false,
     "start_time": "2025-10-21T17:05:27.874320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting fine-tuning for 10 epochs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   4%|▎         | 150/4213 [01:37<45:57,  1.47it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]:  96%|█████████▋| 4065/4213 [50:08<01:50,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]: 100%|██████████| 4213/4213 [51:58<00:00,  1.35it/s]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 527/527 [02:16<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Train Loss: 0.0964 | Val Loss: 0.0894 | P: 0.5001 | R: 0.5430 | F1: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]:  36%|███▌      | 1517/4213 [18:46<33:34,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 2/10 [Train]:  53%|█████▎    | 2220/4213 [27:29<24:52,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 2/10 [Train]: 100%|██████████| 4213/4213 [52:11<00:00,  1.35it/s]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 527/527 [02:10<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] | Train Loss: 0.0893 | Val Loss: 0.0886 | P: 0.5062 | R: 0.5421 | F1: 0.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]:   1%|▏         | 58/4213 [00:45<53:15,  1.30it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 3/10 [Train]: 100%|██████████| 4213/4213 [52:13<00:00,  1.34it/s]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 527/527 [02:15<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] | Train Loss: 0.0887 | Val Loss: 0.0866 | P: 0.5156 | R: 0.5384 | F1: 0.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]:   6%|▌         | 233/4213 [02:54<49:15,  1.35it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 4/10 [Train]: 100%|██████████| 4213/4213 [52:12<00:00,  1.35it/s]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 527/527 [02:11<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] | Train Loss: 0.0883 | Val Loss: 0.0844 | P: 0.5028 | R: 0.5516 | F1: 0.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]:  55%|█████▌    | 2324/4213 [28:47<23:31,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 5/10 [Train]: 100%|█████████▉| 4199/4213 [52:04<00:10,  1.35it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 5/10 [Train]: 100%|██████████| 4213/4213 [52:14<00:00,  1.34it/s]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 527/527 [02:14<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] | Train Loss: 0.0880 | Val Loss: 0.0859 | P: 0.5038 | R: 0.5524 | F1: 0.5071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]:  34%|███▍      | 1429/4213 [17:44<34:31,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 6/10 [Train]:  54%|█████▍    | 2274/4213 [28:11<23:56,  1.35it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 6/10 [Train]: 100%|██████████| 4213/4213 [52:20<00:00,  1.34it/s]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 527/527 [02:12<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] | Train Loss: 0.0877 | Val Loss: 0.0857 | P: 0.5092 | R: 0.5516 | F1: 0.5091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]:   0%|          | 0/4213 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 7/10 [Train]:  33%|███▎      | 1381/4213 [17:10<35:17,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 7/10 [Train]: 100%|██████████| 4213/4213 [52:17<00:00,  1.34it/s]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 527/527 [02:19<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] | Train Loss: 0.0875 | Val Loss: 0.0874 | P: 0.5059 | R: 0.5523 | F1: 0.5075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]:  66%|██████▌   | 2773/4213 [34:23<17:50,  1.35it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 8/10 [Train]: 100%|██████████| 4213/4213 [52:15<00:00,  1.34it/s]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 527/527 [02:11<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] | Train Loss: 0.0873 | Val Loss: 0.0845 | P: 0.5074 | R: 0.5552 | F1: 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]:   9%|▉         | 388/4213 [04:49<47:39,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 9/10 [Train]:  25%|██▍       | 1037/4213 [12:51<39:37,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 9/10 [Train]: 100%|██████████| 4213/4213 [52:14<00:00,  1.34it/s]\n",
      "Epoch 9/10 [Val]: 100%|██████████| 527/527 [02:12<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] | Train Loss: 0.0872 | Val Loss: 0.0844 | P: 0.5126 | R: 0.5508 | F1: 0.5104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]:   1%|▏         | 62/4213 [00:48<52:06,  1.33it/s]/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "Epoch 10/10 [Train]: 100%|██████████| 4213/4213 [52:19<00:00,  1.34it/s]\n",
      "Epoch 10/10 [Val]: 100%|██████████| 527/527 [02:11<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] | Train Loss: 0.0870 | Val Loss: 0.0841 | P: 0.4916 | R: 0.5706 | F1: 0.5083\n",
      "\n",
      " Tuning thresholds on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best thresholds saved to best_thresholds.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 527/527 [02:26<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Test Results (with tuned thresholds)\n",
      "F1 Score: 0.4774\n",
      "Precision: 0.4487\n",
      "Recall: 0.5667\n",
      "\n",
      " Model saved after fine-tuning: model_danboruu_resnet50_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# ======================\n",
    "def find_best_thresholds(y_true, y_pred_proba):\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        p, r, t = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        f1 = 2 * p * r / (p + r + 1e-8)\n",
    "        if len(f1) > 0 and not np.all(np.isnan(f1)):\n",
    "            thresholds.append(t[np.nanargmax(f1)])\n",
    "        else:\n",
    "            thresholds.append(0.2)  # Default fallback threshold\n",
    "    return np.clip(np.array(thresholds, dtype=np.float32), 0.05, 0.95)\n",
    "\n",
    "\n",
    "# ======================\n",
    "history_fine_tuned = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"f1\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "}\n",
    "\n",
    "\n",
    "# ======================\n",
    "num_epochs_fine_tune = 10\n",
    "print(f\" Starting fine-tuning for {num_epochs_fine_tune} epochs...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs_fine_tune):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_fine_tune} [Train]\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ======================\n",
    "    # Validation phase\n",
    "    # ======================\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs_fine_tune} [Val]\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs).cpu()\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    # Use static threshold (0.2) for validation evaluation\n",
    "    preds = (all_probs > 0.2).astype(int)\n",
    "\n",
    "    f1 = f1_score(all_labels, preds, average=\"samples\", zero_division=0)\n",
    "    precision = precision_score(all_labels, preds, average=\"samples\", zero_division=0)\n",
    "    recall = recall_score(all_labels, preds, average=\"samples\", zero_division=0)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_fine_tune}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"P: {precision:.4f} | R: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    history_fine_tuned[\"train_loss\"].append(train_loss)\n",
    "    history_fine_tuned[\"val_loss\"].append(val_loss)\n",
    "    history_fine_tuned[\"precision\"].append(precision)\n",
    "    history_fine_tuned[\"recall\"].append(recall)\n",
    "    history_fine_tuned[\"f1\"].append(f1)\n",
    "\n",
    "\n",
    "print(\"\\n Tuning thresholds on validation set...\")\n",
    "best_thresholds = find_best_thresholds(all_labels, all_probs)\n",
    "np.save(\"best_thresholds.npy\", best_thresholds)\n",
    "print(\" Best thresholds saved to best_thresholds.npy\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "model.eval()\n",
    "all_probs, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.sigmoid(outputs).cpu()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "# Use tuned thresholds for test predictions\n",
    "preds = (all_probs > best_thresholds).astype(int)\n",
    "\n",
    "precision = precision_score(all_labels, preds, average=\"samples\", zero_division=0)\n",
    "recall = recall_score(all_labels, preds, average=\"samples\", zero_division=0)\n",
    "f1 = f1_score(all_labels, preds, average=\"samples\", zero_division=0)\n",
    "\n",
    "print(f\"\\n Final Test Results (with tuned thresholds)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "torch.save(model.state_dict(), \"model_danboruu_resnet50_finetuned.pth\")\n",
    "print(\"\\n Model saved after fine-tuning: model_danboruu_resnet50_finetuned.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 38986,
     "sourceId": 61452,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8527771,
     "sourceId": 13435401,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33544.190986,
   "end_time": "2025-10-22T02:15:08.555248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-21T16:56:04.364262",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
